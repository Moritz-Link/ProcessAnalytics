{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pm4py\n",
    "import graphviz\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.data import Batch\n",
    "#%config Completer.use_jedi = False\n",
    "#\"python.dataScience.enablePlotViewer\": false\n",
    "import networkx as nx \n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from tqdm import  tqdm\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import networkx as nx\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating two different Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASE_ID</th>\n",
       "      <th>ACTIVITY</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>REPAIR_IN_TIME_5D</th>\n",
       "      <th>DEVICETYPE</th>\n",
       "      <th>SERVICEPOINT</th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Case10</td>\n",
       "      <td>Creation</td>\n",
       "      <td>2022-01-02 13:39:47</td>\n",
       "      <td>0</td>\n",
       "      <td>AB52</td>\n",
       "      <td>E</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Case10</td>\n",
       "      <td>Letter</td>\n",
       "      <td>2022-01-05 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AB52</td>\n",
       "      <td>E</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case10</td>\n",
       "      <td>DeviceReceived</td>\n",
       "      <td>2022-01-05 16:45:34</td>\n",
       "      <td>0</td>\n",
       "      <td>AB52</td>\n",
       "      <td>E</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Case10</td>\n",
       "      <td>StockEntry</td>\n",
       "      <td>2022-01-17 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AB52</td>\n",
       "      <td>E</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Case10</td>\n",
       "      <td>InDelivery</td>\n",
       "      <td>2022-01-17 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AB52</td>\n",
       "      <td>E</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CASE_ID        ACTIVITY           TIMESTAMP  REPAIR_IN_TIME_5D DEVICETYPE  \\\n",
       "0  Case10        Creation 2022-01-02 13:39:47                  0       AB52   \n",
       "1  Case10          Letter 2022-01-05 00:00:00                  0       AB52   \n",
       "2  Case10  DeviceReceived 2022-01-05 16:45:34                  0       AB52   \n",
       "3  Case10      StockEntry 2022-01-17 00:00:00                  0       AB52   \n",
       "4  Case10      InDelivery 2022-01-17 00:00:00                  0       AB52   \n",
       "\n",
       "  SERVICEPOINT  YEAR  \n",
       "0            E  2022  \n",
       "1            E  2022  \n",
       "2            E  2022  \n",
       "3            E  2022  \n",
       "4            E  2022  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path = \"Event_log.csv\"\n",
    "df = pd.read_csv(file_path, sep=\";\")\n",
    "df['TIMESTAMP']= pd.to_datetime(df['TIMESTAMP'])\n",
    "df[\"YEAR\"] = df[\"TIMESTAMP\"].dt.year\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASE_ID</th>\n",
       "      <th>ACTIVITY</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>REPAIR_IN_TIME_5D</th>\n",
       "      <th>DEVICETYPE</th>\n",
       "      <th>SERVICEPOINT</th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Case10</td>\n",
       "      <td>Creation</td>\n",
       "      <td>2022-01-02 13:39:47</td>\n",
       "      <td>0</td>\n",
       "      <td>AB52</td>\n",
       "      <td>E</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Case10</td>\n",
       "      <td>Letter</td>\n",
       "      <td>2022-01-05 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AB52</td>\n",
       "      <td>E</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case10</td>\n",
       "      <td>DeviceReceived</td>\n",
       "      <td>2022-01-05 16:45:34</td>\n",
       "      <td>0</td>\n",
       "      <td>AB52</td>\n",
       "      <td>E</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Case10</td>\n",
       "      <td>StockEntry</td>\n",
       "      <td>2022-01-17 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AB52</td>\n",
       "      <td>E</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Case10</td>\n",
       "      <td>InDelivery</td>\n",
       "      <td>2022-01-17 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AB52</td>\n",
       "      <td>E</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178073</th>\n",
       "      <td>Case9999</td>\n",
       "      <td>DeviceReceived</td>\n",
       "      <td>2022-06-22 13:57:52</td>\n",
       "      <td>1</td>\n",
       "      <td>AB34</td>\n",
       "      <td>L</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178074</th>\n",
       "      <td>Case9999</td>\n",
       "      <td>Letter</td>\n",
       "      <td>2022-06-25 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>AB34</td>\n",
       "      <td>L</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178075</th>\n",
       "      <td>Case9999</td>\n",
       "      <td>InDelivery</td>\n",
       "      <td>2022-06-26 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>AB34</td>\n",
       "      <td>L</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178076</th>\n",
       "      <td>Case9999</td>\n",
       "      <td>NoteWorkshop</td>\n",
       "      <td>2022-06-26 10:48:00</td>\n",
       "      <td>1</td>\n",
       "      <td>AB34</td>\n",
       "      <td>L</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178077</th>\n",
       "      <td>Case9999</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2022-06-26 10:48:38</td>\n",
       "      <td>1</td>\n",
       "      <td>AB34</td>\n",
       "      <td>L</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161560 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CASE_ID        ACTIVITY           TIMESTAMP  REPAIR_IN_TIME_5D  \\\n",
       "0         Case10        Creation 2022-01-02 13:39:47                  0   \n",
       "1         Case10          Letter 2022-01-05 00:00:00                  0   \n",
       "2         Case10  DeviceReceived 2022-01-05 16:45:34                  0   \n",
       "3         Case10      StockEntry 2022-01-17 00:00:00                  0   \n",
       "4         Case10      InDelivery 2022-01-17 00:00:00                  0   \n",
       "...          ...             ...                 ...                ...   \n",
       "178073  Case9999  DeviceReceived 2022-06-22 13:57:52                  1   \n",
       "178074  Case9999          Letter 2022-06-25 00:00:00                  1   \n",
       "178075  Case9999      InDelivery 2022-06-26 00:00:00                  1   \n",
       "178076  Case9999    NoteWorkshop 2022-06-26 10:48:00                  1   \n",
       "178077  Case9999       Completed 2022-06-26 10:48:38                  1   \n",
       "\n",
       "       DEVICETYPE SERVICEPOINT  YEAR  \n",
       "0            AB52            E  2022  \n",
       "1            AB52            E  2022  \n",
       "2            AB52            E  2022  \n",
       "3            AB52            E  2022  \n",
       "4            AB52            E  2022  \n",
       "...           ...          ...   ...  \n",
       "178073       AB34            L  2022  \n",
       "178074       AB34            L  2022  \n",
       "178075       AB34            L  2022  \n",
       "178076       AB34            L  2022  \n",
       "178077       AB34            L  2022  \n",
       "\n",
       "[161560 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = df.groupby([\"CASE_ID\", \"SERVICEPOINT\"])#.agg({\"SERVICEPOINT\" : pd.unique})\n",
    "a.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASE_ID                  0\n",
      "ACTIVITY                 0\n",
      "TIMESTAMP                0\n",
      "REPAIR_IN_TIME_5D        0\n",
      "DEVICETYPE              13\n",
      "SERVICEPOINT         16518\n",
      "YEAR                     0\n",
      "dtype: int64\n",
      "1975\n",
      "CASE_ID                  0\n",
      "ACTIVITY                 0\n",
      "TIMESTAMP                0\n",
      "REPAIR_IN_TIME_5D        0\n",
      "DEVICETYPE              13\n",
      "SERVICEPOINT         16518\n",
      "YEAR                     0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16524 entries, 124 to 178024\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   CASE_ID            16524 non-null  object        \n",
      " 1   ACTIVITY           16524 non-null  object        \n",
      " 2   TIMESTAMP          16524 non-null  datetime64[ns]\n",
      " 3   REPAIR_IN_TIME_5D  16524 non-null  int64         \n",
      " 4   DEVICETYPE         16511 non-null  object        \n",
      " 5   SERVICEPOINT       6 non-null      object        \n",
      " 6   YEAR               16524 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(2), object(4)\n",
      "memory usage: 1.0+ MB\n",
      "None\n",
      "1975\n"
     ]
    }
   ],
   "source": [
    "check_nan = df[df[\"SERVICEPOINT\"].isna()]\n",
    "print(check_nan.isna().sum())\n",
    "l = check_nan.CASE_ID.unique()\n",
    "print(len(l))\n",
    "\n",
    "print(df[df[\"CASE_ID\"].isin(l)].isna().sum())\n",
    "print(df[df[\"CASE_ID\"].isin(l)].info())\n",
    "print(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_table = df[[\"CASE_ID\",\t\"ACTIVITY\",\t\"TIMESTAMP\"]]\n",
    "def ceation_activity(x):\n",
    "    \n",
    "    if x.isnull().values.any():\n",
    "        res = np.NaN\n",
    "        a = x.dropna()\n",
    "        if len(a) > 0:\n",
    "            res = a.values[0]\n",
    "\n",
    "        return res\n",
    "    return x.iloc[0]\n",
    "\n",
    "case_table = df.groupby(\"CASE_ID\").agg(\n",
    "    start_time = ('TIMESTAMP', min),\n",
    "    end = ('TIMESTAMP', max),\n",
    "    duration = (\"TIMESTAMP\", lambda x: (max(x) - min(x))),\n",
    "    REPAIR_IN_TIME_5D = (\"REPAIR_IN_TIME_5D\", max),\n",
    "    DEVICETYPE = (\"DEVICETYPE\", lambda x: ceation_activity(x)),\n",
    "    SERVICEPOINT = (\"SERVICEPOINT\", lambda x: ceation_activity(x)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Event_log.csv\"\n",
    "df = pd.read_csv(file_path, sep=\";\")\n",
    "df['TIMESTAMP']= pd.to_datetime(df['TIMESTAMP'])\n",
    "df[\"YEAR\"] = df[\"TIMESTAMP\"].dt.year\n",
    "df.head()\n",
    "\n",
    "unique_case_ids = df.CASE_ID.unique()\n",
    "\n",
    "random_lit_cse_ids = np.random.choice(unique_case_ids, size=500, replace=False)\n",
    "sampled_df = df[df[\"CASE_ID\"].isin(random_lit_cse_ids)  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prefixing():\n",
    "    def __init__(self, case_id_col:str, activity_col:str, time_col:str, general_columns:list,event_columns:list,target_columns:list, cat_columns:list, cat_encoder_type,cat_col_unique_dict:dict, start_event:str = None, end_event:str= None) -> None:\n",
    "        \n",
    "        self.case_id_col            = case_id_col\n",
    "        self.activity_col           = activity_col\n",
    "        self.time_col               = time_col\n",
    "        self.start_event            = start_event\n",
    "        self.end_event              = end_event\n",
    "        self.general_columns        = general_columns\n",
    "        self.cat_columns            = cat_columns\n",
    "        self.event_columns          = event_columns\n",
    "        self.target_columns         = target_columns\n",
    "        self.cat_encoder_type       = cat_encoder_type\n",
    "        self.encoder_dict           = {}\n",
    "        self.cat_col_unique_dict    = cat_col_unique_dict\n",
    "        self.activity_index_dict_for_caseID = None\n",
    "        self.set_encoder()\n",
    "    def get_encoder(self):\n",
    "        return self.encoder_dict\n",
    "    \n",
    "    def get_activity_index_dict_for_caseID(self):\n",
    "        return self.activity_index_dict_for_caseID\n",
    "    def set_encoder(self):\n",
    "        \n",
    "        for cat_col in self.cat_columns:\n",
    "            values = np.array(self.cat_col_unique_dict[cat_col]).reshape(-1,1)\n",
    "            enc = self.cat_encoder_type(handle_unknown='ignore')\n",
    "            enc.fit(values)\n",
    "            self.encoder_dict[cat_col] = enc\n",
    "        return None\n",
    "    def check_prefix_list(self,number_activities_case):\n",
    "        #print(self.prefix_list)\n",
    "        return [value for value in self.prefix_list if value < number_activities_case]\n",
    "    def get_caseID_dict(self, case_id_df_array):\n",
    "        activity_index_dict_for_caseID = {}\n",
    "\n",
    "        for ind in list(range(0,case_id_df_array.shape[0])):\n",
    "            v = case_id_df_array[ind,1]\n",
    "            if v not in list(activity_index_dict_for_caseID.keys()):\n",
    "                activity_index_dict_for_caseID[v] = ind\n",
    "\n",
    "        return activity_index_dict_for_caseID\n",
    "    def get_edge_index_list(self, activity_index_dict_for_caseID,prefix_length_array):\n",
    "        actions_as_int = np.array([activity_index_dict_for_caseID[action] for action in prefix_length_array[:,1]])\n",
    "        \n",
    "\n",
    "        actions_edges = actions_as_int[1:]\n",
    "        actions_as_int = actions_as_int[:-1]\n",
    "        \n",
    "        return np.vstack((actions_as_int, actions_edges))\n",
    "    \n",
    "    def get_general_feature(self,process_df):\n",
    "        return_array = None\n",
    "        n_f = 0\n",
    "        #print(list(process_df.columns))\n",
    "        for pd_col in list(process_df.columns):\n",
    "            f_values = np.array([process_df[pd_col].to_numpy().reshape(-1,1)[0]])\n",
    "            if pd_col in self.cat_columns:\n",
    "                f_values = self.encoder_dict[pd_col].transform(f_values).toarray()\n",
    "            if n_f > 0:\n",
    "                return_array = np.hstack((return_array , f_values))\n",
    "            else: return_array = f_values\n",
    "            n_f += 1\n",
    "\n",
    "        #print(f'GeneraltFeatureArray:.shape{return_array.shape}')\n",
    "        return torch.from_numpy(return_array).float()\n",
    "    def get_event_features(self,event_df):\n",
    "        return_array = None\n",
    "        n_f = 0\n",
    "        #print(list(process_df.columns))\n",
    "        for pd_col in list(event_df.columns):\n",
    "            f_values = event_df[pd_col].to_numpy().reshape(-1,1)\n",
    "            if pd_col in self.cat_columns:\n",
    "                f_values = self.encoder_dict[pd_col].transform(f_values).toarray()\n",
    "            if n_f > 0:\n",
    "                return_array = np.hstack((return_array , f_values))\n",
    "            else: return_array = f_values\n",
    "            n_f += 1\n",
    "\n",
    "        #print(f'EventFeaturesArray:.shape{return_array.shape}')\n",
    "\n",
    "        return torch.from_numpy(return_array)\n",
    "    def transform(self, df:pd.DataFrame,prefix_list :list):\n",
    "        self.prefix_list = prefix_list\n",
    "        graph_list = []\n",
    "        df.sort_values(by=self.time_col, inplace = True)\n",
    "        for case_ind, unique_id in enumerate(tqdm(df[self.case_id_col].unique())):\n",
    "\n",
    "            case_id_df = df[df[self.case_id_col] == unique_id]\n",
    "            event_df = case_id_df[self.event_columns]\n",
    "            process_df = case_id_df[self.general_columns]\n",
    "            process_features = self.get_general_feature(process_df)\n",
    "            event_features = self.get_event_features(event_df)\n",
    "\n",
    "            case_id_df_array = case_id_df.to_numpy()\n",
    "            number_activities_case = len(case_id_df)\n",
    "            prefix_list = self.check_prefix_list(number_activities_case)\n",
    "            activity_index_dict_for_caseID = self.get_caseID_dict(case_id_df_array)\n",
    "            self.activity_index_dict_for_caseID = activity_index_dict_for_caseID\n",
    "            edge_index_list = self.get_edge_index_list(activity_index_dict_for_caseID,case_id_df_array )\n",
    "            y = torch.from_numpy(case_id_df[self.target_columns].to_numpy().astype(int))[0]\n",
    "            for pref_ind , prefix_length in enumerate(prefix_list):\n",
    "                edge_index = torch.from_numpy(edge_index_list[:,0:-(number_activities_case-prefix_length)])\n",
    "                x = event_features[0:-(number_activities_case-prefix_length),:]\n",
    "                graph = Data()\n",
    "                graph.x = x.float()\n",
    "                graph.general_f = process_features\n",
    "                graph.edge_index = edge_index\n",
    "                graph.y = y\n",
    "                graph_list.append(graph)\n",
    "\n",
    "        print(f'Number of Graphs: {len(graph_list)} from  {len(df[self.case_id_col].unique())} Cases')\n",
    "        return graph_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_id_col = \"CASE_ID\"\n",
    "activity_col = \"ACTIVITY\"\n",
    "time_col = \"TIMESTAMP\"\n",
    "cat_col_unique_dict = {\"SERVICEPOINT\": df.SERVICEPOINT.unique(), \"DEVICETYPE\": df.DEVICETYPE.unique(), \"ACTIVITY\": df.ACTIVITY.unique()}\n",
    "target_columns = [\"REPAIR_IN_TIME_5D\"]\n",
    "event_columns = [\"ACTIVITY\"]\n",
    "general_columns = [\"SERVICEPOINT\",\"DEVICETYPE\"]\n",
    "cat_columns = [\"SERVICEPOINT\", \"DEVICETYPE\", \"ACTIVITY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train und Test Split -> Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gesamt: 23906 Cases\n",
      "Ratio 0.9\n",
      "train_size: 21515 Cases\n"
     ]
    }
   ],
   "source": [
    "def train_test(df, train_ratio):#shuffle\n",
    "    train_size = int(df.CASE_ID.nunique() * train_ratio)\n",
    "    print(f'Gesamt: {df.CASE_ID.nunique()} Cases')\n",
    "    print(f'Ratio {train_ratio}')\n",
    "    print(f'train_size: {train_size} Cases')\n",
    "    \n",
    "    unique_case_ids = df.CASE_ID.unique()\n",
    "    random_lit_cse_ids = np.random.choice(unique_case_ids, size=train_size, replace=False)\n",
    "    train_df = df[df[\"CASE_ID\"].isin(random_lit_cse_ids)  ]\n",
    "    test_df = df[~df[\"CASE_ID\"].isin(random_lit_cse_ids)  ]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "#train_df, test_df = train_test(df,0.5)\n",
    "train_df, test_df = train_test(df,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2391/2391 [00:13<00:00, 182.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Graphs: 7430 from  2391 Cases\n"
     ]
    }
   ],
   "source": [
    "# Batch for Test\n",
    "prefix = Prefixing(case_id_col , activity_col,time_col, general_columns,event_columns, target_columns,cat_columns, OneHotEncoder,cat_col_unique_dict )\n",
    "test_graph_list =  prefix.transform(test_df, [2,4,5,8])\n",
    "test_batch = Batch().from_data_list(test_graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23906 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23906/23906 [09:24<00:00, 42.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Graphs: 74611 from  23906 Cases\n",
      "len data: 74611\n",
      "rest: 51\n",
      "number_batches: 1165\n",
      "rest_batch: [[Data(x=[2, 13], general_f=[1, 77], edge_index=[2, 1], y=[1]), Data(x=[4, 13], general_f=[1, 77], edge_index=[2, 3], y=[1]), Data(x=[5, 13], general_f=[1, 77], edge_index=[2, 4], y=[1]), Data(x=[2, 13], general_f=[1, 77], edge_index=[2, 1], y=[1]), Data(x=[4, 13], general_f=[1, 77], edge_index=[2, 3], y=[1]), Data(x=[5, 13], general_f=[1, 77], edge_index=[2, 4], y=[1]), Data(x=[2, 13], general_f=[1, 77], edge_index=[2, 1], y=[1]), Data(x=[4, 13], general_f=[1, 77], edge_index=[2, 3], y=[1]), Data(x=[5, 13], general_f=[1, 77], edge_index=[2, 4], y=[1]), Data(x=[8, 13], general_f=[1, 77], edge_index=[2, 7], y=[1]), Data(x=[2, 13], general_f=[1, 77], edge_index=[2, 1], y=[1]), Data(x=[4, 13], general_f=[1, 77], edge_index=[2, 3], y=[1]), Data(x=[5, 13], general_f=[1, 77], edge_index=[2, 4], y=[1]), Data(x=[2, 13], general_f=[1, 77], edge_index=[2, 1], y=[1]), Data(x=[4, 13], general_f=[1, 77], edge_index=[2, 3], y=[1]), Data(x=[5, 13], general_f=[1, 77], edge_index=[2, 4], y=[1]), Data(x=[8, 13], general_f=[1, 77], edge_index=[2, 7], y=[1]), Data(x=[2, 13], general_f=[1, 77], edge_index=[2, 1], y=[1]), Data(x=[4, 13], general_f=[1, 77], edge_index=[2, 3], y=[1]), Data(x=[5, 13], general_f=[1, 77], edge_index=[2, 4], y=[1]), Data(x=[2, 13], general_f=[1, 77], edge_index=[2, 1], y=[1]), Data(x=[4, 13], general_f=[1, 77], edge_index=[2, 3], y=[1]), Data(x=[5, 13], general_f=[1, 77], edge_index=[2, 4], y=[1]), Data(x=[2, 13], general_f=[1, 77], edge_index=[2, 1], y=[1]), Data(x=[4, 13], general_f=[1, 77], edge_index=[2, 3], y=[1]), Data(x=[5, 13], general_f=[1, 77], edge_index=[2, 4], y=[1]), Data(x=[2, 13], general_f=[1, 77], edge_index=[2, 1], y=[1]), Data(x=[4, 13], general_f=[1, 77], edge_index=[2, 3], y=[1]), Data(x=[5, 13], general_f=[1, 77], edge_index=[2, 4], y=[1]), Data(x=[2, 13], general_f=[1, 77], edge_index=[2, 1], y=[1]), Data(x=[4, 13], general_f=[1, 77], edge_index=[2, 3], y=[1]), Data(x=[5, 13], general_f=[1, 77], edge_index=[2, 4], y=[1]), Data(x=[2, 13], general_f=[1, 77], edge_index=[2, 1], y=[1]), Data(x=[4, 13], general_f=[1, 77], edge_index=[2, 3], y=[1]), Data(x=[5, 13], general_f=[1, 77], edge_index=[2, 4], y=[1]), Data(x=[2, 13], general_f=[1, 77], edge_index=[2, 1], y=[1]), Data(x=[4, 13], general_f=[1, 77], edge_index=[2, 3], y=[1]), Data(x=[5, 13], general_f=[1, 77], edge_index=[2, 4], y=[1]), Data(x=[2, 13], general_f=[1, 77], edge_index=[2, 1], y=[1]), Data(x=[4, 13], general_f=[1, 77], edge_index=[2, 3], y=[1]), Data(x=[5, 13], general_f=[1, 77], edge_index=[2, 4], y=[1]), Data(x=[2, 13], general_f=[1, 77], edge_index=[2, 1], y=[1]), Data(x=[4, 13], general_f=[1, 77], edge_index=[2, 3], y=[1]), Data(x=[5, 13], general_f=[1, 77], edge_index=[2, 4], y=[1]), Data(x=[8, 13], general_f=[1, 77], edge_index=[2, 7], y=[1]), Data(x=[2, 13], general_f=[1, 77], edge_index=[2, 1], y=[1]), Data(x=[4, 13], general_f=[1, 77], edge_index=[2, 3], y=[1]), Data(x=[5, 13], general_f=[1, 77], edge_index=[2, 4], y=[1]), Data(x=[2, 13], general_f=[1, 77], edge_index=[2, 1], y=[1]), Data(x=[4, 13], general_f=[1, 77], edge_index=[2, 3], y=[1]), Data(x=[5, 13], general_f=[1, 77], edge_index=[2, 4], y=[1])]]\n",
      "batches: 74560\n",
      "1166\n"
     ]
    }
   ],
   "source": [
    "# For the Training\n",
    "def generate_batches(graph_data, batch_size):#shuffledd\n",
    "    len_data = len(graph_data)\n",
    "    rest = int(len_data%batch_size)\n",
    "    number_batches= int((len_data-rest) / batch_size) \n",
    "    rest_batch = []\n",
    "    if rest >0:\n",
    "        batches = graph_data[0:-rest:1] \n",
    "        rest_batch = [graph_data[-rest::1]]\n",
    "    else: \n",
    "        batches = graph_data \n",
    "    batches_list = []\n",
    "    for n in range(0,number_batches):\n",
    "        start = n * batch_size\n",
    "        end = start + batch_size\n",
    "\n",
    "        batch = batches[start:end:1]\n",
    "        batches_list.append(batch)\n",
    "    \n",
    "    batches_list += rest_batch\n",
    "    \n",
    "    # rest_batch = Drop the last values in own list\n",
    "    # batches = reshape the remaining into the shape (number_batches X batch_size)\n",
    "    # Append rest_batches to batches \n",
    "\n",
    "    print(f'len data: {len_data}')\n",
    "    print(f'rest: {rest}')\n",
    "    print(f'number_batches: {number_batches}')\n",
    "    print(f'rest_batch: {rest_batch}')\n",
    "    print(f'batches: {len(batches)}')\n",
    "    batches_list_l = []\n",
    "    for bat in batches_list:\n",
    "        batches_list_l.append(Batch().from_data_list(bat))\n",
    "    return batches_list_l\n",
    "\n",
    "#train_batches = generate_batches(res, 4)\n",
    "\n",
    "\n",
    "batches_list = generate_batches(prefix.transform(df, [2,4,5,8]), 64)\n",
    "\n",
    "\n",
    "\n",
    "print(len(batches_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(13, 8)\n",
    "        self.conv2 = GCNConv(8, 8)\n",
    "        self.lin = Linear(8, 8)\n",
    "        self.lin_2 = Linear(8, 1)\n",
    "    def forward(self, data):\n",
    "        x, edge_index,batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        #print(f'x.shape {x.shape}')\n",
    "        x = global_mean_pool(x, batch=batch)\n",
    "        #print(f'global_mean_pool x.shape {x.shape}')\n",
    "        x = self.lin(x)\n",
    "        x = self.lin_2(F.relu(x))\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEmbedding(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gen_lin1 = Linear(77, 16)\n",
    "        self.gen_lin2 = Linear(16, 8)\n",
    "    def forward(self, data):\n",
    "        x, edge_index,batch, general_data = data.x, data.edge_index, data.batch, data.general_f\n",
    "        general_data = self.gen_lin1(general_data)\n",
    "        general_data = F.relu(general_data)\n",
    "        general_data = self.gen_lin2(general_data)\n",
    "        general_data = F.relu(general_data)\n",
    "        return general_data\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(13, 8)\n",
    "        self.conv2 = GCNConv(8, 8)\n",
    "        self.lin = Linear(8, 8)\n",
    "        self.lin_2 = Linear(8, 1)\n",
    "        self.embedding_layer = GCNEmbedding()\n",
    "    def forward(self, data):\n",
    "        x, edge_index,batch, general_data = data.x, data.edge_index, data.batch, data.general_f\n",
    "        general_data = self.embedding_layer(data)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch=batch)\n",
    "        x = x + general_data\n",
    "        x = self.lin(x)\n",
    "        x = self.lin_2(F.relu(x))\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "\n",
    "gcn = GCN()\n",
    "g = batches_list[0]\n",
    "print(g)\n",
    "#gcn(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Loss before training = 0.6753789186477661\n",
      " Test Loss after epoche: 5 = 0.43117618560791016\n"
     ]
    }
   ],
   "source": [
    "model = GCN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCELoss() #torch.nn.CrossEntropyLoss()\n",
    "#batches_list\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    score = 0\n",
    "    for data in batches_list:\n",
    "\n",
    "            out = model(data)  \n",
    "            loss = criterion(out, torch.reshape(data.y.float(), (-1,1)))  # Compute the loss.\n",
    "            score += loss\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return None\n",
    "\n",
    "def test():\n",
    "     model.eval()\n",
    "     test_out = model(test_batch) \n",
    "     loss = criterion(test_out, torch.reshape(test_batch.y.float(), (-1,1)))\n",
    "     return loss\n",
    "\n",
    "t_l = test()\n",
    "print(f' Test Loss before training = {t_l}')\n",
    "for epoch in range(1, 10):\n",
    "    train()\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        t_l = test()\n",
    "        print(f' Test Loss after epoche: {epoch} = {t_l}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
